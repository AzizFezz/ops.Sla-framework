#ce fichier de configuration de prometheus, configure les pod rules, il est √† appliquer avec la commande kubectl apply -f pod-cpu-usage-levels.yaml (et non pas avec helm upgrade)
# J‚Äôai renomm√© ce fichier en "Personnalis√©" pour √©viter d‚Äô√©craser la config d‚Äôorigine extraite apr√®s le d√©ploiement d‚ÄôAlertmanager (qui s‚Äôappelle aussi alertmanager.yaml par d√©faut).
# Pour rappel, on extrait la config actuelle avec :
# minikube kubectl -- get secret alertmanager-prometheus-kube-prometheus-alertmanager -n monitoring -o jsonpath="{.data.alertmanager\.yaml}" | base64 -d > alertmanager.yaml
# Ensuite, il suffit de copier le contenu dans le vrai alertmanager.yaml, puis de r√©encoder avec :
# base64 -w0 alertmanager.yaml > alertmanager.yaml.b64
# (Toutes les commandes d√©taill√©es sont dans le fichier joint au projet)

global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'monitoringk8s@gmail.com'
  smtp_auth_username: 'monitoringk8s@gmail.com'
  smtp_auth_password: 'batk bazm nfmg lutk'
  smtp_require_tls: true

route:
  group_by: ['alertname']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 1h
  receiver: 'null'  # Par d√©faut, rien n'est envoy√©

  routes:
    # Ignorer les alertes Watchdog
    - matchers:
        - alertname="Watchdog"
      receiver: "null"

    # Alertes CPU normales
    - matchers:
        - alertname="PodCpuNormal"
      receiver: "mail-and-slo-tracker"
      repeat_interval: 2h
      group_interval: 10m
      continue: true

    # Alertes CPU √©lev√©es
    - matchers:
        - alertname=~"PodHighCpuUsage|PodCpuFailure"
      receiver: "mail-and-slo-tracker"
      repeat_interval: 30m
      continue: true

    # Alertes m√©moire
    - matchers:
        - alertname="PodHighMemoryUsage"
      receiver: "mail-and-slo-tracker"
      repeat_interval: 1h
      continue: true

    # Webhooks pour toutes les alertes CPU
    - matchers:
        - alertname=~"PodCpuNormal|PodHighCpuUsage|PodCpuFailure"
      receiver: "flask-webhook"
      continue: true

    # SLO Tracker pour alertes critiques uniquement
    - matchers:
        - alertname=~"PodHighCpuUsage|PodHighMemoryUsage|PodCpuFailure"
      receiver: "slo-tracker-webhook"

receivers:
  - name: "null"

  - name: "mail-and-slo-tracker"
    email_configs:
      - to: 'monitoringk8s@gmail.com'
        send_resolved: false
        html: |
          <h3>üö® Alerte Kubernetes - {{ .CommonLabels.alertname }}</h3>
          <p><b>R√©sum√©:</b> {{ .CommonAnnotations.summary }}</p>
          <p><b>Description:</b> {{ .CommonAnnotations.description }}</p>
          <p><b>Pods concern√©s :</b></p>
          <ul>
            {{ range .Alerts }}
              <li><b>{{ .Labels.pod }}</b> (namespace: {{ .Labels.namespace }})</li>
            {{ end }}
          </ul>
          <p><i>Notification limit√©e selon la criticit√© de l'alerte.</i></p>
    webhook_configs:
      - url: 'http://192.168.46.133:8080/api/v1/incident/1/webhook/prometheus'
        send_resolved: true

  - name: 'flask-webhook'
    webhook_configs:
      - url: 'http://192.168.46.249:5000/alert'
        send_resolved: true

  - name: 'slo-tracker-webhook'
    webhook_configs:
      - url: 'http://192.168.46.133:8080/api/v1/incident/1/webhook/prometheus'
        send_resolved: true

inhibit_rules:
  - source_matchers:
      - severity="critical"
    target_matchers:
      - severity="warning"
    equal:
      - alertname
      - pod
      - namespace

  - source_matchers:
      - alertname=~"PodHighCpuUsage|PodCpuFailure"
    target_matchers:
      - alertname="PodCpuNormal"
    equal:
      - pod
      - namespace

templates:
  - /etc/alertmanager/config/*.tmpl
