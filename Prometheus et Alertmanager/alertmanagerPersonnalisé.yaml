#ce fichier de configuration de prometheus, configure les pod rules, il est √† appliquer avec la commande kubectl apply -f pod-cpu-usage-levels.yaml (et non pas avec helm upgrade)
# J‚Äôai renomm√© ce fichier en "Personnalis√©" pour √©viter d‚Äô√©craser la config d‚Äôorigine extraite apr√®s le d√©ploiement d‚ÄôAlertmanager (qui s‚Äôappelle aussi alertmanager.yaml par d√©faut).
# Pour rappel, on extrait la config actuelle avec :
# minikube kubectl -- get secret alertmanager-prometheus-kube-prometheus-alertmanager -n monitoring -o jsonpath="{.data.alertmanager\.yaml}" | base64 -d > alertmanager.yaml
# Ensuite, il suffit de copier le contenu dans le vrai alertmanager.yaml, puis de r√©encoder avec :
# base64 -w0 alertmanager.yaml > alertmanager.yaml.b64
# (Toutes les commandes d√©taill√©es sont dans le fichier joint au projet)

global:
  resolve_timeout: 5m  # D√©lai avant de consid√©rer une alerte comme r√©solue
  smtp_smarthost: 'smtp.gmail.com:587'  # Serveur SMTP utilis√© pour l'envoi des mails
  smtp_from: 'monitoringk8s@gmail.com'  # Adresse d'exp√©dition des alertes email
  smtp_auth_username: 'monitoringk8s@gmail.com'  # Identifiant SMTP (adresse email)
  smtp_auth_password: 'batk bazm nfmg lutk'  # Mot de passe d‚Äôapplication Gmail (voir rapport pour la proc√©dure de cr√©ation)
  smtp_require_tls: true  # Connexion s√©curis√©e TLS obligatoire

route:
  group_by: ['alertname']  # Grouper les alertes par nom
  group_wait: 30s          # Attendre 30s avant d‚Äôenvoyer un groupe d‚Äôalertes
  group_interval: 5m       # Attendre 5min entre deux groupes d‚Äôalertes similaires
  repeat_interval: 1h      # R√©p√©ter l‚Äôalerte toutes les heures si elle persiste
  receiver: 'null'         # Par d√©faut, aucune notification n‚Äôest envoy√©e

  routes:
    # Ignore les alertes Watchdog (pour √©viter le spam)
    - matchers:
        - alertname="Watchdog"
      receiver: "null"

    # Alertes CPU normales
    - matchers:
        - alertname="PodCpuNormal"
      receiver: "mail-and-slo-tracker"
      repeat_interval: 2h
      group_interval: 10m
      continue: true

    # Alertes CPU √©lev√©es
    - matchers:
        - alertname=~"PodHighCpuUsage|PodCpuFailure"
      receiver: "mail-and-slo-tracker"
      repeat_interval: 30m
      continue: true

    # Alertes m√©moire
    - matchers:
        - alertname="PodHighMemoryUsage"
      receiver: "mail-and-slo-tracker"
      repeat_interval: 1h
      continue: true

    # Webhooks pour toutes les alertes CPU
    - matchers:
        - alertname=~"PodCpuNormal|PodHighCpuUsage|PodCpuFailure"
      receiver: "flask-webhook"
      continue: true

    # SLO Tracker pour alertes critiques uniquement
    - matchers:
        - alertname=~"PodHighCpuUsage|PodHighMemoryUsage|PodCpuFailure"
      receiver: "slo-tracker-webhook"

receivers:
  - name: "null"  # Receiver vide, utilis√© pour ignorer certaines alertes

  - name: "mail-and-slo-tracker"
    email_configs:
      - to: 'monitoringk8s@gmail.com'  # Adresse de r√©ception des alertes (modifiable)
        send_resolved: false
        html: |
          <h3>üö® Alerte Kubernetes - {{ .CommonLabels.alertname }}</h3>
          <p><b>R√©sum√©:</b> {{ .CommonAnnotations.summary }}</p>
          <p><b>Description:</b> {{ .CommonAnnotations.description }}</p>
          <p><b>Pods concern√©s :</b></p>
          <ul>
            {{ range .Alerts }}
              <li><b>{{ .Labels.pod }}</b> (namespace: {{ .Labels.namespace }})</li>
            {{ end }}
          </ul>
          <p><i>Notification limit√©e selon la criticit√© de l'alerte.</i></p>
    webhook_configs:
      - url: 'http://192.168.46.133:8080/api/v1/incident/1/webhook/prometheus'  # Webhook vers une app sur une autre machine du r√©seau

  - name: 'flask-webhook'
    webhook_configs:
      - url: 'http://192.168.46.249:5000/alert'  # Webhook vers une app Flask sur une autre machine du r√©seau
        # Si l'application Flask est d√©ploy√©e sur la m√™me machine que l'Alertmanager, remplacer l'IP par localhost :
        # url: 'http://localhost:5000/alert'

  - name: 'slo-tracker-webhook'
    webhook_configs:
      - url: 'http://192.168.46.133:8080/api/v1/incident/1/webhook/prometheus'  # Webhook r√©cup√©r√© dans SLO tracker (section "Int√©grations" > "Ajouter Prometheus" > copier le lien fourni)
        # M√™me si le SLO tracker est sur la m√™me machine, il faut utiliser le lien fourni par l'interface SLO tracker (ne pas remplacer par localhost sauf indication contraire dans la doc SLO tracker)

inhibit_rules:
  # Inhibe (masque) les alertes warning si une alerte critical existe pour le m√™me pod/namespace
  - source_matchers:
      - severity="critical"
    target_matchers:
      - severity="warning"
    equal:
      - alertname
      - pod
      - namespace

  # Inhibe les alertes PodCpuNormal si une alerte critique CPU existe d√©j√† pour le m√™me pod/namespace
  - source_matchers:
      - alertname=~"PodHighCpuUsage|PodCpuFailure"
    target_matchers:
      - alertname="PodCpuNormal"
    equal:
      - pod
      - namespace

templates:
  - /etc/alertmanager/config/*.tmpl  # Fichiers de template pour personnaliser les emails/messages
